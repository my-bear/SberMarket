{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listwise ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next purchase pridiction\n",
    "\n",
    "This is a competition notebook for [SberMarket Inclass challenge](https://www.kaggle.com/c/sbermarket-internship-competition).\n",
    "\n",
    "The dataset is the purchase history of twenty thousand customers for a couple of years. The data contains statistics for each purchase (without specifying the quantity of purchased goods), there are only the list of purchased categories (a total of 881 categories).\n",
    "\n",
    "The challenge is to predict the next order, **regardless of the time of the next order** and regardless of the number of purchased items of each category.<br>\n",
    "\n",
    "In the basic ranking tutorial, we trained a model that can predict ratings for user/categories pairs. The model was trained to minimize the mean squared error of predicted ratings.\n",
    "\n",
    "We do not need ranking models to predict scores with great accuracy. Instead, we care more about the ability of the model to generate an ordered list of items that matches the user's preference ordering.\n",
    "\n",
    "Instead of optimizing the model's predictions on individual query/item pairs, we can optimize the model's ranking of a list as a whole. This method is called listwise ranking.\n",
    "\n",
    "In this tutorial, we will use TensorFlow Recommenders to build listwise ranking models. To do so, we will make use of ranking losses and metrics provided by TensorFlow Ranking, a TensorFlow package that focuses on learning to rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If TensorFlow Ranking is not available in your runtime environment, you can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow-recommenders\n",
    "# pip install tensorflow-ranking\n",
    "# pip install --upgrade tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then import all the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "from sklearn import preprocessing\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\n",
    "    os.path.join('..','data','train.csv'))\n",
    "\n",
    "sub = pd.read_csv(\n",
    "    os.path.join('..','data','sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training dataset creation and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```order_number``` - order counter for each user<br>\n",
    "```user_id``` - user ID<br>\n",
    "```category``` - category<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если пользователь заказал один раз он не попадет в список последних заказов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sparse matrix for temporary use\n",
    "train_raw = pd.get_dummies(raw, columns = ['cart'], prefix='', prefix_sep='', dtype='bool')\n",
    "train_raw = train_raw.groupby(['user_id', 'order_completed_at']).any().reset_index()\n",
    "\n",
    "# order counter for each use\n",
    "train_raw['order_number'] = train_raw.groupby(['user_id']).cumcount()\n",
    "train_raw = train_raw.drop('order_completed_at', axis=1)\n",
    "\n",
    "# separate datasets by the last purchase\n",
    "last_order = train_raw.groupby(['user_id'])['order_number'].transform(max) == train_raw['order_number']\n",
    "train = train_raw[~last_order].groupby('user_id').sum().reset_index()\n",
    "train = train.drop('order_number', axis=1)\n",
    "valid = train_raw[last_order].groupby('user_id').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_melt = pd.melt(train, id_vars=['user_id'],var_name='category', value_name='count')\n",
    "train_melt = train_melt[train_melt['count']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total purchase counter for each user\n",
    "order_number = valid[['user_id', 'order_number']].set_index('user_id').squeeze()\n",
    "valid = valid.drop('order_number', axis=1)\n",
    "train_melt['orders_total']= train_melt['user_id'].map(order_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_melt = pd.melt(valid, id_vars=['user_id'],var_name='category', value_name='count')\n",
    "valid_melt = valid_melt[valid_melt['count']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-3653b2435b4b>:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  valid_melt = valid_melt.drop('count', 1)\n"
     ]
    }
   ],
   "source": [
    "valid_melt = valid_melt.drop('count', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average amount of each category in  customer's purchase\n",
    "train_melt['rating'] = train_melt['count'] / train_melt['orders_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_melt = train_melt.drop(['orders_total', 'count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = train_melt.to_dict('records')\n",
    "valid_dict = valid_melt.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17610280</th>\n",
       "      <td>10280</td>\n",
       "      <td>880</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17613281</th>\n",
       "      <td>13281</td>\n",
       "      <td>880</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17613965</th>\n",
       "      <td>13965</td>\n",
       "      <td>880</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616977</th>\n",
       "      <td>16977</td>\n",
       "      <td>880</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17617384</th>\n",
       "      <td>17384</td>\n",
       "      <td>880</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1031269 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id category    rating\n",
       "2               2        0  0.071429\n",
       "8               8        0  0.142857\n",
       "9               9        0  0.022222\n",
       "12             12        0  0.050000\n",
       "13             13        0  0.187500\n",
       "...           ...      ...       ...\n",
       "17610280    10280      880  0.250000\n",
       "17613281    13281      880  0.333333\n",
       "17613965    13965      880  0.250000\n",
       "17616977    16977      880  0.100000\n",
       "17617384    17384      880  0.400000\n",
       "\n",
       "[1031269 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1031269.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rating\n",
       "count  1031269.00\n",
       "mean         0.28\n",
       "std          0.23\n",
       "min          0.00\n",
       "25%          0.10\n",
       "50%          0.20\n",
       "75%          0.40\n",
       "max          1.00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_melt[['rating']].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVuUlEQVR4nO3df7DldX3f8ecr/MgPL7qBNatBYNUstZSpDdwgmsTem9QRqA6xbiesVpQm3SpqjRMmpk4KZTLp6NTa4GyUbgxRU7s3jliLDDaxZq9IIlGWILDiD6ISt4tSlvDjoGIW3v3jHPTm7v1x7t37PYe7n+dj5s4953x/vd/s5bzO98f5fFNVSJLa9UPjLkCSNF4GgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCNSfJ3iRT465DeqIwCHTESfL1JP9s3muvSXIDQFX9o6qaXWYdm5NUkqM7LFV6QjAIpDEwYPREYhCoOXP3GJKcleSmJA8m+VaSdw5mu37w+/4kvSTPT/JDSX4ryV1J7knygSRPmbPeCwfTDiT5D/O28x+TfDjJf0/yIPCawbY/k+T+JHcn2ZHk2DnrqyQXJ/lKkoeS/HaSZw+WeTDJh+bOL62WQaDWXQFcUVVPBp4NfGjw+gsHvzdU1URVfQZ4zeBnGngWMAHsAEhyGvBu4JXA04GnACfO29b5wIeBDcAHgUeBNwMbgecDvwhcPG+Zc4AzgbOB3wB2DrZxEnA6sO0wepcAg0BHro8OPmnfn+R++m/SC/k74KeSbKyqXlXduMQ6Xwm8s6q+WlU94N8DFwwO82wFPlZVN1TV94BLgfkDeX2mqj5aVY9V1Xeqak9V3VhVB6vq68B/A/7pvGXeXlUPVtVe4HbgTwfbfwD4OPDTw/8nkRZmEOhI9UtVteHxHw79pP24XwFOBb6Y5HNJXrLEOn8SuGvO87uAo4FNg2nfeHxCVX0bODBv+W/MfZLk1CTXJvnm4HDRf6K/dzDXt+Y8/s4CzyeWqFcaikGgplXVV6pqG/ATwNuBDyd5Eod+mgfYD5wy5/nJwEH6b853A894fEKSHwVOmL+5ec/fA3wR2DI4NPVWIKvvRlodg0BNS/Kvkjy1qh4D7h+8/Cjw/4DH6J8LeNwu4M1Jnplkgv4n+D+uqoP0j/2/NMkLBidwL2f5N/XjgAeBXpLnAK9bs8akFTAI1LpzgL1JevRPHF9QVd8dHNr5HeDPB+cZzgauAv6I/hVFXwO+C7wRYHAM/43ADP29g4eAe4BHltj2JcArBvP+PvDHa9+etLx4Yxpp7Q32GO6nf9jna+OuR1qKewTSGkny0iQ/NjjH8A7gNuDr461KWp5BIK2d8+mfUN4PbKF/mMldbj3heWhIkhrnHoEkNW7dDXy1cePG2rx584qXe/jhh3nSk5609gU9gdlzG1rsGdrs+3B63rNnz71V9dSFpq27INi8eTM33XTTipebnZ1lampq7Qt6ArPnNrTYM7TZ9+H0nOSuxaZ5aEiSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhq37r5ZfDim3z89tm3vfvXusW1bkpbiHoEkNc4gkKTGGQSS1LjOgiDJSUl2J7kjyd4kb1pgnqkkDyS5ZfBzaVf1SJIW1uXJ4oPAr1fVzUmOA/Yk+URVfWHefJ+uqpd0WIckaQmd7RFU1d1VdfPg8UPAHcCJXW1PkrQ6I7lncZLNwPXA6VX14JzXp4CrgX30b/h9SVXtXWD57cB2gE2bNp05MzOz4hp6vR77H9m/iurXxqknnDrybfZ6PSYmJka+3XGy53a02Pfh9Dw9Pb2nqiYXmtZ5ECSZAD4F/E5VfWTetCcDj1VVL8l5wBVVtWWp9U1OTtZq71B2+V2Xr3i5tTKO7xF4B6c2tNgztNn3Yd6hbNEg6PQLZUmOof+J/4PzQwBg7t5BVV2X5N1JNlbVvV3WNQ7j+DLbtoltTDE18u1KWl+6vGoowB8Ad1TVOxeZ52mD+Uhy1qCeA13VJEk6VJd7BD8LvAq4Lcktg9feCpwMUFVXAluB1yU5CHwHuKBGcdJCkvR9nQVBVd0AZJl5dgA7uqpBkrQ8v1ksSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjOguCJCcl2Z3kjiR7k7xpgXmS5F1J7kxya5IzuqpHkrSwoztc90Hg16vq5iTHAXuSfKKqvjBnnnOBLYOf5wHvGfyWJI1IZ3sEVXV3Vd08ePwQcAdw4rzZzgc+UH03AhuSPL2rmiRJh0pVdb+RZDNwPXB6VT045/VrgbdV1Q2D558E3lJVN81bfjuwHWDTpk1nzszMrLiGXq/H/kf2r7aFden4o45n44aN4y5jpHq9HhMTE+MuY6Ra7Bna7Ptwep6ent5TVZMLTevy0BAASSaAq4FfmxsCj09eYJFDkqmqdgI7ASYnJ2tqamrFdczOzrLrwK4VL7eebZvYxtapreMuY6RmZ2dZzd/HetZiz9Bm31313OlVQ0mOoR8CH6yqjywwyz7gpDnPnwG09bFdksasy6uGAvwBcEdVvXOR2a4BLhxcPXQ28EBV3d1VTZKkQ3V5aOhngVcBtyW5ZfDaW4GTAarqSuA64DzgTuDbwEUd1iNJWkBnQTA4AbzQOYC58xTw+q5qkCQtz28WS1LjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUuM6CIMlVSe5Jcvsi06eSPJDklsHPpV3VIkla3NEdrvt9wA7gA0vM8+mqekmHNUiSltHZHkFVXQ/c19X6JUlrI1U13IzJk6rq4RWtPNkMXFtVpy8wbQq4GtgH7Acuqaq9i6xnO7AdYNOmTWfOzMyspAwAer0e+x/Zv+Ll1rPjjzqejRs2jruMker1ekxMTIy7jJFqsWdos+/D6Xl6enpPVU0uNG3ZQ0NJXgC8F5gATk7yXODfVtXFq6rmB24GTqmqXpLzgI8CWxaasap2AjsBJicna2pqasUbm52dZdeBXauvdh3aNrGNrVNbx13GSM3OzrKav4/1rMWeoc2+u+p5mEND/xV4MXAAoKo+D7zwcDdcVQ9WVW/w+DrgmCRtfXyVpCeAoc4RVNU35r306OFuOMnTkmTw+KxBLQcOd72SpJUZ5qqhbwwOD1WSY4F/B9yx3EJJdgFTwMYk+4DLgGMAqupKYCvwuiQHge8AF9SwJywkSWtmmCB4LXAFcCL9E7t/Crx+uYWqatsy03fQv7xUkjRGywZBVd0LvHIEtUiSxmCYq4b+EDjkkE1V/etOKpIkjdQwh4aunfP4R4CX0b/uX5J0BBjm0NDVc58PTgL/n84qkiSN1GqGmNgCnLzWhUiSxmOYcwQP0T9HkMHvbwJv6bguSdKIDHNo6LhRFCJJGo9FgyDJGUstWFU3r305kqRRW2qP4L8sMa2AX1jjWiRJY7BoEFTV9CgLkSSNx1B3KEtyOnAa/e8RAFBVS915TJK0Tgxz1dBl9AePOw24DjgXuIGlb0EpSVonhtkj2Ao8F/irqrooySb6N6rROjD9/vEd4dv96t1j2W6LPUuHY5gg+G5VPZbkYJInA/cAz+q4Lh0BxvGGvG1iyUFvJS1gqctHdwC7gM8m2QD8PrAH6AGfHU15kqSuLbVH8BXgHcBP0n/z3wW8CHhyVd06gtokSSOw6FhDVXVFVT2f/v2J7wP+EPg48EtJFrzJvCRp/Vl20Lmququq3l5VPw28gv4w1F/svDJJ0kgsGwRJjkny0iQfpL9H8GXg5Z1XJkkaiaVOFr8I2Ab8c/onh2eA7VX18IhqkySNwFIni98K/A/gkqq6b0T1SJJGzLGGJKlxq7lDmSTpCGIQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhrXWRAkuSrJPUluX2R6krwryZ1Jbk1yRle1SJIW1+UewfuAc5aYfi6wZfCzHXhPh7VIkhbRWRBU1fX0b2izmPOBD1TfjcCGJE/vqh5J0sJSVd2tPNkMXFtVpy8w7VrgbVV1w+D5J4G3VNVNC8y7nf5eA5s2bTpzZmZmxbX0ej32P7J/xcutZ8cfdTz3PdrWwLHj7vnUE04d+TZ7vR4TExMj3+64tdj34fQ8PT29p6omF5q21DDUXcsCry2YSlW1E9gJMDk5WVNTUyve2OzsLLsO7FrxcuvZtolt7OrZ8yjtfvnukW9zdnaW1fw/sd612HdXPY/zqqF9wElznj8DaOsjuyQ9AYwzCK4BLhxcPXQ28EBV3T3GeiSpSZ0dGkqyC5gCNibZB1wGHANQVVcC1wHnAXcC3wYu6qoWSdLiOguCqtq2zPQCXt/V9iVJw/GbxZLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNa7TIEhyTpIvJbkzyW8uMH0qyQNJbhn8XNplPZKkQx3d1YqTHAX8HvAiYB/wuSTXVNUX5s366ap6SVd1SJKW1uUewVnAnVX11ar6HjADnN/h9iRJq5Cq6mbFyVbgnKr61cHzVwHPq6o3zJlnCria/h7DfuCSqtq7wLq2A9sBNm3adObMzMyK6+n1eux/ZP8qOlm/jj/qeO579L5xlzFS4+751BNOHfk2e70eExMTI9/uuLXY9+H0PD09vaeqJhea1tmhISALvDY/dW4GTqmqXpLzgI8CWw5ZqGonsBNgcnKypqamVlzM7Owsuw7sWvFy69m2iW3s6tnzKO1++e6Rb3N2dpbV/D+x3rXYd1c9d3loaB9w0pznz6D/qf/7qurBquoNHl8HHJNkY4c1SZLm6TIIPgdsSfLMJMcCFwDXzJ0hydOSZPD4rEE9BzqsSZI0T2eHhqrqYJI3AH8CHAVcVVV7k7x2MP1KYCvwuiQHge8AF1RXJy0kSQvq8hzB44d7rpv32pVzHu8AdnRZgyRpaX6zWJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxnX6PQJJozH9/umxbHf3q0c/tpLWnkEgSSswrtAFuOyUyzpZr4eGJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuO8fFRaQ+O4tHDbxLaRb1NHFvcIJKlx7hFIWrUj8ctVLXKPQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGuc3iyWtS18+8GUuf//l4y7jiOAegSQ1ziCQpMYZBJLUuE6DIMk5Sb6U5M4kv7nA9CR512D6rUnO6LIeSdKhOguCJEcBvwecC5wGbEty2rzZzgW2DH62A+/pqh5J0sK63CM4C7izqr5aVd8DZoDz581zPvCB6rsR2JDk6R3WJEmap8vLR08EvjHn+T7geUPMcyJw99yZkmynv8cA0EvypVXUsxG4dxXLrVuzzNpzA1rsGdrs+zB7PmWxCV0GQRZ4rVYxD1W1E9h5WMUkN1XV5OGsY72x5za02DO02XdXPXd5aGgfcNKc588A9q9iHklSh7oMgs8BW5I8M8mxwAXANfPmuQa4cHD10NnAA1V19/wVSZK609mhoao6mOQNwJ8ARwFXVdXeJK8dTL8SuA44D7gT+DZwUVf1cJiHltYpe25Diz1Dm3130nOqDjkkL0lqiN8slqTGGQSS1LgjLghaHNZiiJ5fOej11iR/keS546hzLS3X85z5fibJo0m2jrK+LgzTc5KpJLck2ZvkU6Ouca0N8bf9lCQfS/L5Qc9dnmcciSRXJbknye2LTF/797CqOmJ+6J+U/mvgWcCxwOeB0+bNcx7wcfrfYTgb+Mtx1z2Cnl8A/Pjg8bkt9Dxnvj+jf1HC1nHXPYJ/5w3AF4CTB89/Ytx1j6DntwJvHzx+KnAfcOy4az/Mvl8InAHcvsj0NX8PO9L2CFoc1mLZnqvqL6rqbwdPb6T/fY31bJh/Z4A3AlcD94yyuI4M0/MrgI9U1d8AVNV673uYngs4LkmACfpBcHC0Za6tqrqefh+LWfP3sCMtCBYbsmKl86wnK+3nV+h/mljPlu05yYnAy4ArR1hXl4b5dz4V+PEks0n2JLlwZNV1Y5iedwD/kP4XUW8D3lRVj42mvLFZ8/ewI+1WlWs2rMU6MnQ/SabpB8HPdVpR94bp+XeBt1TVo/0Pi+veMD0fDZwJ/CLwo8BnktxYVV/uuriODNPzi4FbgF8Ang18Ismnq+rBrosbozV/DzvSgqDFYS2G6ifJPwbeC5xbVQdGVFtXhul5EpgZhMBG4LwkB6vqo6Mpcc0N+7d9b1U9DDyc5HrgucB6DYJher4IeFv1D57fmeRrwHOAz46mxLFY8/ewI+3QUIvDWizbc5KTgY8Ar1rHnw7nWrbnqnpmVW2uqs3Ah4GL13EIwHB/2/8L+PkkRyf5Mfqj/d4x4jrX0jA9/w39PSCSbAL+AfDVkVY5emv+HnZE7RHUE29Yi84N2fOlwAnAuwefkA/WOh61cciejyjD9FxVdyT538CtwGPAe6tqwUsQ14Mh/51/G3hfktvoHzJ5S1Wt66Gpk+wCpoCNSfYBlwHHQHfvYQ4xIUmNO9IODUmSVsggkKTGGQSS1DiDQJIaZxBIUuMMAmmOJE9LMpPkr5N8Icl1SU5dw/VPJXnBWq1PWgsGgTQwGLjsfwKzVfXsqjqN/uiWm9ZwM1P0R4OVnjAMAukHpoG/m/uFtKq6BbghyX9OcnuS25L8Mnz/0/21j8+bZEeS1wwefz3J5UluHizznCSbgdcCbx7cM+Dnk/zLwXo/PxgSQhq5I+qbxdJhOh3Ys8Dr/wL4J/TH7dkIfG7IN+17q+qMJBcDl1TVrya5EuhV1TsABt+IfXFV/d8kG9amDWll3COQlvdzwK6qerSqvgV8CviZIZb7yOD3HmDzIvP8Of0hEv4N/WEUpJEzCKQf2Et/GOf5FhvH+iB///+hH5k3/ZHB70dZZO+7ql4L/Bb90SRvSXLC0NVKa8QgkH7gz4AfHnw6B/r3PAb+FvjlJEcleSr9Wwl+FrgLOC3JDyd5CoNRMJfxEHDcnPU/u6r+sqouBe7l7w8vLI2E5wikgaqqJC8Dfndwo/TvAl8Hfo3+bRA/T/8GIL9RVd8ESPIh+qN9fgX4qyE28zHgw0nOp38rzTcn2UJ/r+OTg21II+Xoo5LUOA8NSVLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUuP8PCv3f+yl6A+EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(train_melt['rating'], 10, density=True, facecolor='g', alpha=0.75)\n",
    "\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Histogram')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_set, Test_set = train_test_split(train_dict, test_size = 0.2,\n",
    "                                        stratify = None, random_state = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trein : 825015 , Test : 206254 \n"
     ]
    }
   ],
   "source": [
    "print(f'Trein : {len(Train_set)} , Test : {len(Test_set)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_category = np.unique(train_melt.category).astype('str')\n",
    "unique_user_ids = np.unique(train_raw.user_id).astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we cannot use the MovieLens dataset for list optimization directly. To perform listwise optimization, we need to have access to a list of movies each user has rated, but each example in the MovieLens 100K dataset contains only the rating of a single movie.\n",
    "\n",
    "To get around this we transform the dataset so that each example contains a user id and a list of movies rated by that user. Some movies in the list will be ranked higher than others; the goal of our model will be to make predictions that match this ordering.\n",
    "\n",
    "To do this, we use the tfrs.examples.movielens.movielens_to_listwise helper function. It takes the MovieLens 100K dataset and generates a dataset containing list examples as discussed above. The implementation details can be found in the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import collections\n",
    "from typing import Dict, List, Optional, Text, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_feature_dict() -> Dict[Text, List[tf.Tensor]]:\n",
    "    \"\"\"Helper function for creating an empty feature dict for defaultdict.\"\"\"\n",
    "    return {\"category\": [], \"user_rating\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_list(\n",
    "    feature_lists: Dict[Text, List[tf.Tensor]],\n",
    "    num_examples_per_list: int,\n",
    "    random_state: Optional[np.random.RandomState] = None,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \n",
    "    \"\"\"Function for sampling a list example from given feature lists.\"\"\"\n",
    "    \n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState()\n",
    "\n",
    "    sampled_indices = random_state.choice(\n",
    "      range(len(feature_lists[\"category\"])),\n",
    "      size=num_examples_per_list,\n",
    "      replace=False,\n",
    "    )\n",
    "    sampled_movie_titles = [\n",
    "      feature_lists[\"category\"][idx] for idx in sampled_indices\n",
    "    ]\n",
    "    sampled_ratings = [\n",
    "      feature_lists[\"user_rating\"][idx] for idx in sampled_indices\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "      tf.concat(sampled_movie_titles, 0),\n",
    "      tf.concat(sampled_ratings, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_listwise(\n",
    "    rating_dataset: tf.data.Dataset,\n",
    "    num_list_per_user: int = 10,\n",
    "    num_examples_per_list: int = 10,\n",
    "    seed: Optional[int] = None,\n",
    ") -> tf.data.Dataset:\n",
    "    \n",
    "    \"\"\"Function for converting the MovieLens 100K dataset to a listwise dataset.\n",
    "      Args:\n",
    "          rating_dataset:\n",
    "            The MovieLens ratings dataset loaded from TFDS with features\n",
    "            \"movie_title\", \"user_id\", and \"user_rating\".\n",
    "          num_list_per_user:\n",
    "            An integer representing the number of lists that should be sampled for\n",
    "            each user in the training dataset.\n",
    "          num_examples_per_list:\n",
    "            An integer representing the number of movies to be sampled for each list\n",
    "            from the list of movies rated by the user.\n",
    "          seed:\n",
    "            An integer for creating `np.random.RandomState`.\n",
    "      Returns:\n",
    "          A tf.data.Dataset containing list examples.\n",
    "          Each example contains three keys: \"user_id\", \"movie_title\", and\n",
    "          \"user_rating\". \"user_id\" maps to a string tensor that represents the user\n",
    "          id for the example. \"movie_title\" maps to a tensor of shape\n",
    "          [sum(num_example_per_list)] with dtype tf.string. It represents the list\n",
    "          of candidate movie ids. \"user_rating\" maps to a tensor of shape\n",
    "          [sum(num_example_per_list)] with dtype tf.float32. It represents the\n",
    "          rating of each movie in the candidate list.\n",
    "      \"\"\"\n",
    "    \n",
    "    random_state = np.random.RandomState(seed)\n",
    "    example_lists_by_user = collections.defaultdict(_create_feature_dict)\n",
    "\n",
    "    category_vocab = set()\n",
    "\n",
    "    for example in train_dict:\n",
    "\n",
    "        user_id = str(example[\"user_id\"])\n",
    "\n",
    "        example_lists_by_user[user_id][\"category\"].append(\n",
    "            tf.constant(str(example['category'])))\n",
    "            \n",
    "        example_lists_by_user[user_id][\"user_rating\"].append(\n",
    "            tf.constant(example[\"rating\"], dtype=tf.float64))\n",
    "\n",
    "        category_vocab.add(str(example[\"category\"]))\n",
    "\n",
    "    tensor_slices = {\"user_id\": [], \"category\": [], \"user_rating\": []}\n",
    "\n",
    "    for user_id, feature_lists in example_lists_by_user.items():\n",
    "        for _ in range(num_list_per_user):\n",
    "\n",
    "          # Drop the user if they don't have enough ratings.\n",
    "            if len(feature_lists[\"category\"]) < num_examples_per_list:\n",
    "                continue\n",
    "\n",
    "            sampled_category, sampled_ratings = _sample_list(\n",
    "              feature_lists,\n",
    "              num_examples_per_list,\n",
    "              random_state=random_state\n",
    "            )\n",
    "\n",
    "            tensor_slices[\"user_id\"].append(user_id)\n",
    "            tensor_slices[\"category\"].append(sampled_category)\n",
    "            tensor_slices[\"user_rating\"].append(sampled_ratings)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices(tensor_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train = sample_listwise(\n",
    "    Train_set,\n",
    "    num_list_per_user=50,\n",
    "    num_examples_per_list=20,\n",
    "    seed=42\n",
    ")\n",
    "test = sample_listwise(\n",
    "    Test_set,\n",
    "    num_list_per_user=1,\n",
    "    num_examples_per_list=20,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': <tf.Tensor: shape=(20,), dtype=string, numpy=\n",
      "array([b'15', b'409', b'57', b'0', b'99', b'820', b'24', b'157', b'29',\n",
      "       b'395', b'420', b'134', b'23', b'376', b'16', b'82', b'398',\n",
      "       b'382', b'139', b'168'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'2'>,\n",
      " 'user_rating': <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([0.21428571, 0.35714286, 0.71428571, 0.07142857, 0.07142857,\n",
      "       0.07142857, 0.14285714, 0.14285714, 0.14285714, 0.07142857,\n",
      "       0.14285714, 0.14285714, 0.71428571, 0.14285714, 0.14285714,\n",
      "       0.5       , 0.28571429, 0.28571429, 0.07142857, 0.07142857])>}\n",
      "{'category': <tf.Tensor: shape=(20,), dtype=string, numpy=\n",
      "array([b'85', b'381', b'379', b'129', b'425', b'5', b'406', b'16', b'134',\n",
      "       b'64', b'55', b'399', b'43', b'61', b'100', b'158', b'14', b'396',\n",
      "       b'431', b'57'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'2'>,\n",
      " 'user_rating': <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
      "       0.14285714, 0.07142857, 0.14285714, 0.14285714, 0.07142857,\n",
      "       0.07142857, 0.07142857, 0.07142857, 0.57142857, 0.07142857,\n",
      "       0.07142857, 0.07142857, 0.21428571, 0.07142857, 0.71428571])>}\n",
      "{'category': <tf.Tensor: shape=(20,), dtype=string, numpy=\n",
      "array([b'432', b'41', b'55', b'23', b'17', b'412', b'409', b'219', b'168',\n",
      "       b'43', b'379', b'395', b'393', b'129', b'67', b'157', b'61', b'99',\n",
      "       b'15', b'85'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'2'>,\n",
      " 'user_rating': <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([0.14285714, 0.14285714, 0.07142857, 0.71428571, 0.14285714,\n",
      "       0.14285714, 0.35714286, 0.07142857, 0.07142857, 0.07142857,\n",
      "       0.07142857, 0.07142857, 0.14285714, 0.07142857, 0.21428571,\n",
      "       0.14285714, 0.57142857, 0.07142857, 0.21428571, 0.07142857])>}\n",
      "{'category': <tf.Tensor: shape=(20,), dtype=string, numpy=\n",
      "array([b'403', b'64', b'29', b'23', b'384', b'67', b'406', b'99', b'412',\n",
      "       b'139', b'402', b'42', b'377', b'199', b'54', b'408', b'219',\n",
      "       b'203', b'88', b'393'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'2'>,\n",
      " 'user_rating': <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([0.5       , 0.07142857, 0.14285714, 0.71428571, 0.07142857,\n",
      "       0.21428571, 0.07142857, 0.07142857, 0.14285714, 0.07142857,\n",
      "       0.14285714, 0.07142857, 0.07142857, 0.14285714, 0.14285714,\n",
      "       0.07142857, 0.07142857, 0.07142857, 0.14285714, 0.14285714])>}\n",
      "{'category': <tf.Tensor: shape=(20,), dtype=string, numpy=\n",
      "array([b'29', b'199', b'219', b'158', b'396', b'402', b'43', b'398',\n",
      "       b'383', b'25', b'61', b'17', b'399', b'24', b'22', b'0', b'425',\n",
      "       b'432', b'165', b'804'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'2'>,\n",
      " 'user_rating': <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([0.14285714, 0.14285714, 0.07142857, 0.07142857, 0.21428571,\n",
      "       0.14285714, 0.07142857, 0.28571429, 0.21428571, 0.07142857,\n",
      "       0.57142857, 0.14285714, 0.07142857, 0.14285714, 0.28571429,\n",
      "       0.07142857, 0.07142857, 0.14285714, 0.14285714, 0.28571429])>}\n"
     ]
    }
   ],
   "source": [
    "for example in train.take(5):\n",
    "    pprint.pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_id_user = valid_melt['user_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "category_shape = []\n",
    "\n",
    "for id_user in valid_id_user:\n",
    "    \n",
    "    category = tf.constant(valid_melt[valid_melt['user_id'] == id_user]['category'].values)\n",
    "    id_user = tf.constant(str(id_user))\n",
    "    \n",
    "    category_shape.append(category.shape[0])\n",
    "    val.append({'user_id':id_user, 'category':[category]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'7'>,\n",
       "  'category': [<tf.Tensor: shape=(9,), dtype=string, numpy=\n",
       "   array([b'0', b'5', b'16', b'20', b'22', b'90', b'230', b'379', b'393'],\n",
       "         dtype=object)>]},\n",
       " {'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'12'>,\n",
       "  'category': [<tf.Tensor: shape=(18,), dtype=string, numpy=\n",
       "   array([b'0', b'14', b'19', b'22', b'27', b'29', b'32', b'57', b'67',\n",
       "          b'379', b'380', b'388', b'395', b'397', b'398', b'419', b'437',\n",
       "          b'443'], dtype=object)>]},\n",
       " {'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'29'>,\n",
       "  'category': [<tf.Tensor: shape=(17,), dtype=string, numpy=\n",
       "   array([b'0', b'5', b'17', b'55', b'57', b'63', b'87', b'100', b'112',\n",
       "          b'146', b'382', b'394', b'395', b'402', b'409', b'420', b'430'],\n",
       "         dtype=object)>]},\n",
       " {'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'36'>,\n",
       "  'category': [<tf.Tensor: shape=(9,), dtype=string, numpy=\n",
       "   array([b'0', b'11', b'22', b'57', b'61', b'396', b'440', b'443', b'445'],\n",
       "         dtype=object)>]},\n",
       " {'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'39'>,\n",
       "  'category': [<tf.Tensor: shape=(18,), dtype=string, numpy=\n",
       "   array([b'0', b'14', b'15', b'19', b'22', b'26', b'57', b'84', b'86',\n",
       "          b'87', b'99', b'379', b'383', b'393', b'394', b'396', b'401',\n",
       "          b'804'], dtype=object)>]}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect an example from the training data. The example includes a user id, a list of 10 movie ids, and their ratings by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the same model with three different losses:\n",
    "\n",
    "* mean squared error,\n",
    "* pairwise hinge loss, and\n",
    "* a listwise ListMLE loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three losses correspond to pointwise, pairwise, and listwise optimization.\n",
    "\n",
    "To evaluate the model we use [normalized discounted cumulative gain (NDCG)](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG). NDCG measures a predicted ranking by taking a weighted sum of the actual rating of each candidate. The ratings of movies that are ranked lower by the model would be discounted more. As a result, a good model that ranks highly-rated movies on top would have a high NDCG result. Since this metric takes the ranked position of each candidate into account, it is a listwise metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS OF THE EMBEDDING LAYER of tf.keras.layers.Embedding\n",
    "\n",
    "**'input_dim'** = the vocab size that we will choose. In other words it is the number of unique words in the vocab.\n",
    "\n",
    "**'output_dim'** = the number of dimensions we wish to embed into. Each word will be represented by a vector of this much dimensions.\n",
    "\n",
    "**'input_length'** = lenght of the maximum document. which is stored in maxlen variable in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"unique_user_ids = array([b'1', b'10', b'100', b'101', b'102', b'103', ... ])\"\"\"\n",
    "        \n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # Compute embeddings for users.\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_user_ids),\n",
    "          tf.keras.layers.Embedding(len(unique_user_ids) + 2, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Compute embeddings for movies.\n",
    "        self.category_embeddings = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_category),\n",
    "          tf.keras.layers.Embedding(len(unique_category) + 2, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Compute predictions.\n",
    "        self.score_model = tf.keras.Sequential([\n",
    "          # Learn multiple dense layers.  \n",
    "          tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "          # Make rating predictions in the final layer.\n",
    "          tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "          loss=loss,\n",
    "          metrics=[\n",
    "            tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "            tf.keras.metrics.RootMeanSquaredError()\n",
    "          ]\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        # We first convert the id features into embeddings.\n",
    "        # User embeddings are a [batch_size, embedding_dim] tensor.\n",
    "        user_embeddings = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "        # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
    "        # tensor.\n",
    "        category_embeddings = self.category_embeddings(features[\"category\"])\n",
    "\n",
    "        # We want to concatenate user embeddings with movie emebeddings to pass\n",
    "        # them into the ranking model. To do so, we need to reshape the user\n",
    "        # embeddings to match the shape of movie embeddings.\n",
    "        list_length = features[\"category\"].shape[1]\n",
    "        user_embedding_repeated = tf.repeat(\n",
    "            tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "        # Once reshaped, we concatenate and pass into the dense layers to generate\n",
    "        # predictions.\n",
    "        concatenated_embeddings = tf.concat(\n",
    "            [user_embedding_repeated, category_embeddings], 2)\n",
    " \n",
    "        return self.score_model(concatenated_embeddings)\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        labels = features.pop(\"user_rating\")\n",
    "\n",
    "        scores = self(features)\n",
    "\n",
    "        return self.task(\n",
    "            labels=labels,\n",
    "            predictions=tf.squeeze(scores, axis=-1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listwise_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train each of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is very similar to the model in the basic ranking tutorial. We train the model to minimize the mean squared error between the actual ratings and predicted ratings. Therefore, this loss is computed individually for each movie and the training is pointwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_model = RankingModel(tf.keras.losses.MeanSquaredError())\n",
    "mse_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x105c53610d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_model.fit(cached_train, epochs=epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise hinge loss model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By minimizing the pairwise hinge loss, the model tries to maximize the difference between the model's predictions for a highly rated item and a low rated item: the bigger that difference is, the lower the model loss. However, once the difference is large enough, the loss becomes zero, stopping the model from further optimizing this particular pair and letting it focus on other pairs that are incorrectly ranked\n",
    "\n",
    "This loss is not computed for individual movies, but rather for pairs of movies. Hence the training using this loss is pairwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinge_model = RankingModel(tfr.keras.losses.PairwiseHingeLoss())\n",
    "hinge_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x105c2a20040>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hinge_model.fit(cached_train, epochs=epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listwise model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ListMLE loss from TensorFlow Ranking expresses list maximum likelihood estimation. To calculate the ListMLE loss, we first use the user ratings to generate an optimal ranking. We then calculate the likelihood of each candidate being out-ranked by any item below it in the optimal ranking using the predicted scores. The model tries to minimize such likelihood to ensure highly rated candidates are not out-ranked by low rated candidates. You can learn more about the details of ListMLE in section 2.2 of the paper Position-aware ListMLE: A Sequential Learning Process.\n",
    "\n",
    "Note that since the likelihood is computed with respect to a candidate and all candidates below it in the optimal ranking, the loss is not pairwise but listwise. Hence the training uses list optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "106/106 - 16s - ndcg_metric: 0.8747 - root_mean_squared_error: 0.4378 - loss: 41.8320 - regularization_loss: 0.0000e+00 - total_loss: 41.8320 - 16s/epoch - 147ms/step\n",
      "Epoch 2/40\n",
      "106/106 - 9s - ndcg_metric: 0.8800 - root_mean_squared_error: 0.4141 - loss: 41.8499 - regularization_loss: 0.0000e+00 - total_loss: 41.8499 - 9s/epoch - 81ms/step\n",
      "Epoch 3/40\n",
      "106/106 - 9s - ndcg_metric: 0.8801 - root_mean_squared_error: 0.4109 - loss: 41.8432 - regularization_loss: 0.0000e+00 - total_loss: 41.8432 - 9s/epoch - 81ms/step\n",
      "Epoch 4/40\n",
      "106/106 - 9s - ndcg_metric: 0.8801 - root_mean_squared_error: 0.4387 - loss: 41.8247 - regularization_loss: 0.0000e+00 - total_loss: 41.8247 - 9s/epoch - 81ms/step\n",
      "Epoch 5/40\n",
      "106/106 - 9s - ndcg_metric: 0.8803 - root_mean_squared_error: 0.5367 - loss: 41.7991 - regularization_loss: 0.0000e+00 - total_loss: 41.7991 - 9s/epoch - 81ms/step\n",
      "Epoch 6/40\n",
      "106/106 - 9s - ndcg_metric: 0.8810 - root_mean_squared_error: 0.8116 - loss: 41.5199 - regularization_loss: 0.0000e+00 - total_loss: 41.5199 - 9s/epoch - 81ms/step\n",
      "Epoch 7/40\n",
      "106/106 - 9s - ndcg_metric: 0.8831 - root_mean_squared_error: 1.2582 - loss: 41.3379 - regularization_loss: 0.0000e+00 - total_loss: 41.3379 - 9s/epoch - 81ms/step\n",
      "Epoch 8/40\n",
      "106/106 - 9s - ndcg_metric: 0.8852 - root_mean_squared_error: 1.4510 - loss: 41.2611 - regularization_loss: 0.0000e+00 - total_loss: 41.2611 - 9s/epoch - 81ms/step\n",
      "Epoch 9/40\n",
      "106/106 - 9s - ndcg_metric: 0.8875 - root_mean_squared_error: 1.5556 - loss: 41.0797 - regularization_loss: 0.0000e+00 - total_loss: 41.0797 - 9s/epoch - 81ms/step\n",
      "Epoch 10/40\n",
      "106/106 - 9s - ndcg_metric: 0.8912 - root_mean_squared_error: 1.6978 - loss: 40.8819 - regularization_loss: 0.0000e+00 - total_loss: 40.8819 - 9s/epoch - 81ms/step\n",
      "Epoch 11/40\n",
      "106/106 - 9s - ndcg_metric: 0.8968 - root_mean_squared_error: 1.9163 - loss: 40.6525 - regularization_loss: 0.0000e+00 - total_loss: 40.6525 - 9s/epoch - 81ms/step\n",
      "Epoch 12/40\n",
      "106/106 - 9s - ndcg_metric: 0.9050 - root_mean_squared_error: 2.1951 - loss: 40.1929 - regularization_loss: 0.0000e+00 - total_loss: 40.1929 - 9s/epoch - 82ms/step\n",
      "Epoch 13/40\n",
      "106/106 - 9s - ndcg_metric: 0.9139 - root_mean_squared_error: 2.4701 - loss: 39.5759 - regularization_loss: 0.0000e+00 - total_loss: 39.5759 - 9s/epoch - 82ms/step\n",
      "Epoch 14/40\n",
      "106/106 - 9s - ndcg_metric: 0.9225 - root_mean_squared_error: 2.7395 - loss: 39.1851 - regularization_loss: 0.0000e+00 - total_loss: 39.1851 - 9s/epoch - 82ms/step\n",
      "Epoch 15/40\n",
      "106/106 - 9s - ndcg_metric: 0.9307 - root_mean_squared_error: 3.0109 - loss: 38.6052 - regularization_loss: 0.0000e+00 - total_loss: 38.6052 - 9s/epoch - 82ms/step\n",
      "Epoch 16/40\n",
      "106/106 - 9s - ndcg_metric: 0.9382 - root_mean_squared_error: 3.2606 - loss: 38.3788 - regularization_loss: 0.0000e+00 - total_loss: 38.3788 - 9s/epoch - 82ms/step\n",
      "Epoch 17/40\n",
      "106/106 - 9s - ndcg_metric: 0.9448 - root_mean_squared_error: 3.5065 - loss: 37.6243 - regularization_loss: 0.0000e+00 - total_loss: 37.6243 - 9s/epoch - 82ms/step\n",
      "Epoch 18/40\n",
      "106/106 - 9s - ndcg_metric: 0.9503 - root_mean_squared_error: 3.7458 - loss: 37.2870 - regularization_loss: 0.0000e+00 - total_loss: 37.2870 - 9s/epoch - 82ms/step\n",
      "Epoch 19/40\n",
      "106/106 - 9s - ndcg_metric: 0.9550 - root_mean_squared_error: 3.9427 - loss: 36.7495 - regularization_loss: 0.0000e+00 - total_loss: 36.7495 - 9s/epoch - 82ms/step\n",
      "Epoch 20/40\n",
      "106/106 - 9s - ndcg_metric: 0.9591 - root_mean_squared_error: 4.1282 - loss: 36.3595 - regularization_loss: 0.0000e+00 - total_loss: 36.3595 - 9s/epoch - 83ms/step\n",
      "Epoch 21/40\n",
      "106/106 - 9s - ndcg_metric: 0.9624 - root_mean_squared_error: 4.3019 - loss: 36.2173 - regularization_loss: 0.0000e+00 - total_loss: 36.2173 - 9s/epoch - 83ms/step\n",
      "Epoch 22/40\n",
      "106/106 - 9s - ndcg_metric: 0.9652 - root_mean_squared_error: 4.4646 - loss: 36.0125 - regularization_loss: 0.0000e+00 - total_loss: 36.0125 - 9s/epoch - 83ms/step\n",
      "Epoch 23/40\n",
      "106/106 - 9s - ndcg_metric: 0.9676 - root_mean_squared_error: 4.6086 - loss: 35.4675 - regularization_loss: 0.0000e+00 - total_loss: 35.4675 - 9s/epoch - 83ms/step\n",
      "Epoch 24/40\n",
      "106/106 - 9s - ndcg_metric: 0.9696 - root_mean_squared_error: 4.7538 - loss: 35.3266 - regularization_loss: 0.0000e+00 - total_loss: 35.3266 - 9s/epoch - 83ms/step\n",
      "Epoch 25/40\n",
      "106/106 - 9s - ndcg_metric: 0.9713 - root_mean_squared_error: 4.8735 - loss: 35.0036 - regularization_loss: 0.0000e+00 - total_loss: 35.0036 - 9s/epoch - 83ms/step\n",
      "Epoch 26/40\n",
      "106/106 - 9s - ndcg_metric: 0.9727 - root_mean_squared_error: 4.9736 - loss: 34.7191 - regularization_loss: 0.0000e+00 - total_loss: 34.7191 - 9s/epoch - 83ms/step\n",
      "Epoch 27/40\n",
      "106/106 - 9s - ndcg_metric: 0.9741 - root_mean_squared_error: 5.0689 - loss: 34.5593 - regularization_loss: 0.0000e+00 - total_loss: 34.5593 - 9s/epoch - 83ms/step\n",
      "Epoch 28/40\n",
      "106/106 - 9s - ndcg_metric: 0.9752 - root_mean_squared_error: 5.1629 - loss: 34.5652 - regularization_loss: 0.0000e+00 - total_loss: 34.5652 - 9s/epoch - 83ms/step\n",
      "Epoch 29/40\n",
      "106/106 - 9s - ndcg_metric: 0.9763 - root_mean_squared_error: 5.2107 - loss: 34.3801 - regularization_loss: 0.0000e+00 - total_loss: 34.3801 - 9s/epoch - 83ms/step\n",
      "Epoch 30/40\n",
      "106/106 - 9s - ndcg_metric: 0.9771 - root_mean_squared_error: 5.2610 - loss: 34.2720 - regularization_loss: 0.0000e+00 - total_loss: 34.2720 - 9s/epoch - 83ms/step\n",
      "Epoch 31/40\n",
      "106/106 - 9s - ndcg_metric: 0.9780 - root_mean_squared_error: 5.3201 - loss: 34.2131 - regularization_loss: 0.0000e+00 - total_loss: 34.2131 - 9s/epoch - 83ms/step\n",
      "Epoch 32/40\n",
      "106/106 - 9s - ndcg_metric: 0.9787 - root_mean_squared_error: 5.3742 - loss: 33.8387 - regularization_loss: 0.0000e+00 - total_loss: 33.8387 - 9s/epoch - 83ms/step\n",
      "Epoch 33/40\n",
      "106/106 - 9s - ndcg_metric: 0.9793 - root_mean_squared_error: 5.3842 - loss: 33.7309 - regularization_loss: 0.0000e+00 - total_loss: 33.7309 - 9s/epoch - 83ms/step\n",
      "Epoch 34/40\n",
      "106/106 - 9s - ndcg_metric: 0.9799 - root_mean_squared_error: 5.4132 - loss: 33.6668 - regularization_loss: 0.0000e+00 - total_loss: 33.6668 - 9s/epoch - 83ms/step\n",
      "Epoch 35/40\n",
      "106/106 - 9s - ndcg_metric: 0.9805 - root_mean_squared_error: 5.4574 - loss: 33.4286 - regularization_loss: 0.0000e+00 - total_loss: 33.4286 - 9s/epoch - 83ms/step\n",
      "Epoch 36/40\n",
      "106/106 - 9s - ndcg_metric: 0.9810 - root_mean_squared_error: 5.4732 - loss: 33.3496 - regularization_loss: 0.0000e+00 - total_loss: 33.3496 - 9s/epoch - 83ms/step\n",
      "Epoch 37/40\n",
      "106/106 - 9s - ndcg_metric: 0.9815 - root_mean_squared_error: 5.5058 - loss: 33.2377 - regularization_loss: 0.0000e+00 - total_loss: 33.2377 - 9s/epoch - 83ms/step\n",
      "Epoch 38/40\n",
      "106/106 - 9s - ndcg_metric: 0.9819 - root_mean_squared_error: 5.5240 - loss: 33.1467 - regularization_loss: 0.0000e+00 - total_loss: 33.1467 - 9s/epoch - 83ms/step\n",
      "Epoch 39/40\n",
      "106/106 - 9s - ndcg_metric: 0.9823 - root_mean_squared_error: 5.5389 - loss: 33.1365 - regularization_loss: 0.0000e+00 - total_loss: 33.1365 - 9s/epoch - 83ms/step\n",
      "Epoch 40/40\n",
      "106/106 - 9s - ndcg_metric: 0.9827 - root_mean_squared_error: 5.5508 - loss: 33.0109 - regularization_loss: 0.0000e+00 - total_loss: 33.0109 - 9s/epoch - 86ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27fcc334dc0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listwise_model.fit(cached_train, epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 33ms/step - ndcg_metric: 0.8976 - root_mean_squared_error: 0.2395 - loss: 0.0549 - regularization_loss: 0.0000e+00 - total_loss: 0.0549\n",
      "NDCG of the MSE Model: 0.8976\n"
     ]
    }
   ],
   "source": [
    "mse_model_result = mse_model.evaluate(cached_test, return_dict=True)\n",
    "print(\"NDCG of the MSE Model: {:.4f}\".format(mse_model_result[\"ndcg_metric\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 28ms/step - ndcg_metric: 0.9314 - root_mean_squared_error: 3.6647 - loss: 0.9645 - regularization_loss: 0.0000e+00 - total_loss: 0.9645\n",
      "NDCG of the pairwise hinge loss model: 0.9314\n"
     ]
    }
   ],
   "source": [
    "hinge_model_result = hinge_model.evaluate(cached_test, return_dict=True)\n",
    "print(\"NDCG of the pairwise hinge loss model: {:.4f}\".format(hinge_model_result[\"ndcg_metric\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 41ms/step - ndcg_metric: 0.9811 - root_mean_squared_error: 5.7217 - loss: 33.4156 - regularization_loss: 0.0000e+00 - total_loss: 33.4156\n",
      "NDCG of the ListMLE model: 0.9811\n"
     ]
    }
   ],
   "source": [
    "listwise_model_result = listwise_model.evaluate(cached_test, return_dict=True)\n",
    "print(\"NDCG of the ListMLE model: {:.4f}\".format(listwise_model_result[\"ndcg_metric\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the three models, the model trained using ListMLE has the highest NDCG metric. This result shows how listwise optimization can be used to train ranking models and can potentially produce models that perform better than models optimized in a pointwise or pairwise fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "listwise_model.save_weights(\n",
    "    os.path.join(os.path.pardir+os.sep+'weights'+os.sep+'model-weights')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcb94c54c10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listwise_model.load_weights(os.path.join(os.path.pardir+os.sep+'weights'+os.sep+'model-weights'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List to recommendation to validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_recommends(model, list_id, list_item, count_rec):\n",
    "    \n",
    "    recommend=[]\n",
    "    \n",
    "    if np.array(count_rec).shape[0] == list_id.shape[0]:\n",
    "        pass\n",
    "    else:\n",
    "        count_rec = count_rec[0:list_id.shape[0]]\n",
    "        \n",
    "    for id_, count_recomendation in zip(list_id, count_rec):\n",
    "        \n",
    "        inputs = {\n",
    "            \"user_id\":\n",
    "                tf.expand_dims(str(id_), axis=0),\n",
    "            \"category\":\n",
    "                tf.expand_dims(list_item, axis=0)}\n",
    "        \n",
    "        scores = model(inputs)\n",
    "        \n",
    "        cat = tfr.utils.sort_by_scores(np.squeeze(scores,axis=-1),\n",
    "                                  [tf.expand_dims(list_item, axis=0)])[0]\n",
    "        \n",
    "        recommend.append({'user_id':id_, 'category':cat[0,:count_recomendation]})\n",
    "\n",
    "    return recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_list = to_recommends(listwise_model, valid_id_user, unique_category, category_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': 7,\n",
       "  'category': <tf.Tensor: shape=(9,), dtype=string, numpy=\n",
       "  array([b'100', b'15', b'398', b'84', b'250', b'19', b'396', b'388', b'43'],\n",
       "        dtype=object)>},\n",
       " {'user_id': 12,\n",
       "  'category': <tf.Tensor: shape=(18,), dtype=string, numpy=\n",
       "  array([b'55', b'425', b'395', b'167', b'31', b'379', b'14', b'100',\n",
       "         b'391', b'383', b'54', b'104', b'707', b'724', b'880', b'29',\n",
       "         b'19', b'23'], dtype=object)>},\n",
       " {'user_id': 29,\n",
       "  'category': <tf.Tensor: shape=(17,), dtype=string, numpy=\n",
       "  array([b'84', b'57', b'61', b'17', b'398', b'409', b'55', b'402', b'712',\n",
       "         b'14', b'22', b'100', b'380', b'41', b'430', b'16', b'19'],\n",
       "        dtype=object)>}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'7'>,\n",
       "  'category': [<tf.Tensor: shape=(9,), dtype=string, numpy=\n",
       "   array([b'0', b'5', b'16', b'20', b'22', b'90', b'230', b'379', b'393'],\n",
       "         dtype=object)>]},\n",
       " {'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'12'>,\n",
       "  'category': [<tf.Tensor: shape=(18,), dtype=string, numpy=\n",
       "   array([b'0', b'14', b'19', b'22', b'27', b'29', b'32', b'57', b'67',\n",
       "          b'379', b'380', b'388', b'395', b'397', b'398', b'419', b'437',\n",
       "          b'443'], dtype=object)>]},\n",
       " {'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'29'>,\n",
       "  'category': [<tf.Tensor: shape=(17,), dtype=string, numpy=\n",
       "   array([b'0', b'5', b'17', b'55', b'57', b'63', b'87', b'100', b'112',\n",
       "          b'146', b'382', b'394', b'395', b'402', b'409', b'420', b'430'],\n",
       "         dtype=object)>]}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.296\n"
     ]
    }
   ],
   "source": [
    "score_binary = []\n",
    "\n",
    "for category_true, category_pred in zip(val,rec_list):\n",
    "    \n",
    "    y_pred = np.array(category_pred['category'])\n",
    "    y_true = np.array(category_true['category'])[0]\n",
    "    \n",
    "    for x in y_pred:\n",
    "        if x in y_true:\n",
    "            score_binary.append(1)\n",
    "        else:\n",
    "            score_binary.append(0)\n",
    "            \n",
    "res = round(sum(score_binary)/len(score_binary), 3)\n",
    "                  \n",
    "print(f'accuracy: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List to recommendation to submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0;133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0;5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0;10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0;396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0;14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target\n",
       "0  0;133       0\n",
       "1    0;5       1\n",
       "2   0;10       0\n",
       "3  0;396       1\n",
       "4   0;14       0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id =[]\n",
    "cats = []\n",
    "\n",
    "for row in sub.id.values:\n",
    "    user, cat = row.split(';')\n",
    "    user_id.append(int(user))\n",
    "    cats.append(cat)\n",
    "    \n",
    "sub_list = pd.DataFrame({'id': user_id, 'category':cats})\n",
    "\n",
    "sub_id_user = sub_list['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ = []\n",
    "sub_category_shape = []\n",
    "\n",
    "for id_user in sub_id_user:\n",
    "    \n",
    "    category = tf.constant(sub_list[sub_list['id'] == id_user]['category'].values)\n",
    "    id_user = tf.constant(str(id_user))\n",
    "    \n",
    "    sub_category_shape.append(category.shape[0])\n",
    "    sub_.append({'user_id':id_user, 'category':[category]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_rec_list = to_recommends(listwise_model, sub_id_user, unique_category, sub_category_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.346\n"
     ]
    }
   ],
   "source": [
    "score_binary = []\n",
    "\n",
    "for category_true, category_pred in zip(sub_,sub_rec_list):\n",
    "    \n",
    "    y_pred = np.array(category_pred['category'])\n",
    "    y_true = np.array(category_true['category'])[0]\n",
    "    \n",
    "    for x in y_pred:\n",
    "        if x in y_true:\n",
    "            score_binary.append(1)\n",
    "        else:\n",
    "            score_binary.append(0)\n",
    "            \n",
    "res = round(sum(score_binary)/len(score_binary), 3)\n",
    "                  \n",
    "print(f'accuracy: {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0;133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0;5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0;10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0;396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0;14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target\n",
       "0  0;133       1\n",
       "1    0;5       0\n",
       "2   0;10       1\n",
       "3  0;396       0\n",
       "4   0;14       0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = sub.copy()\n",
    "submission['target'] = score_binary\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
